# Controlling Assembly Output from C Code

## Table of Contents
1. [Introduction](#introduction)
2. [Inline Assembly Basics](#inline-assembly-basics)
3. [Register Control](#register-control)
4. [Function Attributes](#function-attributes)
5. [Data Type Control](#data-type-control)
6. [Compiler Flags](#compiler-flags)
7. [Memory Barriers and Ordering](#memory-barriers-and-ordering)
8. [Practical Examples](#practical-examples)
9. [Debugging and Verification](#debugging-and-verification)

## Introduction

When writing performance-critical code, you often need precise control over the assembly instructions generated by the compiler. This guide covers techniques to influence how `clang` (and GCC) translate C code into assembly.

## Inline Assembly Basics

### Basic Syntax

```c
__asm__ [volatile] (
    "assembly code"
    : output operands (optional)
    : input operands (optional)
    : clobber list (optional)
);
```

### Clobber List

A clobber list tells the compiler which registers, memory, or condition codes your inline assembly modifies (or
  "clobbers"). This prevents the compiler from assuming those resources still contain their original values after your
  assembly code runs.

  Syntax

  The clobber list is the fourth part of extended inline assembly:

  __asm__ volatile(
      "assembly code"
      : outputs
      : inputs
      : clobber list  // <-- This part
  );

  Common Clobbers

  1. Registers

  __asm__ volatile(
      "movl $42, %%eax\n"
      "movl $10, %%ebx"
      :
      :
      : "eax", "ebx"  // Tell compiler we modified eax and ebx
  );

  2. Memory

  // "memory" means the assembly might read/write any memory location
  __asm__ volatile(
      "movl %0, (%1)"
      :
      : "r"(value), "r"(ptr)
      : "memory"  // We're writing to memory
  );

  3. Condition Codes

  // "cc" means we modified the flags register (condition codes)
  __asm__ volatile(
      "cmp %0, %1"
      :
      : "r"(a), "r"(b)
      : "cc"  // CMP instruction modifies flags
  );

  Why Clobbers Matter

  Without proper clobbers, the compiler might:
  - Keep values in registers that your assembly overwrites
  - Reorder memory operations incorrectly
  - Make wrong assumptions about flag states

  Bad Example (missing clobber):
  int x = 5;
  __asm__ volatile("movl $0, %%eax");  // No clobber list!
  // Compiler might still think eax contains something important

  Good Example:
  int x = 5;
  __asm__ volatile(
      "movl $0, %%eax"
      ::: "eax"  // Tell compiler we trashed eax
  );

  Special Cases

  - Don't list input/output registers - They're automatically considered clobbered
  - "memory" is conservative - Use it when in doubt about memory effects
  - x86-64 has more registers - "rax" vs "eax", etc.

  For your multiply chain, if you're only modifying the input/output registers, you might not need additional clobbers,
  but "cc" could be needed since imul sets flags.

### The `volatile` Keyword

`volatile` prevents the compiler from optimizing away or reordering the assembly:

```c
// Without volatile - compiler may optimize away if result unused
__asm__("movl $42, %eax");

// With volatile - always executed
__asm__ volatile("movl $42, %eax");
```

### Extended Assembly with Constraints

```c
int multiply(int a, int b) {
    int result;
    __asm__ volatile(
        "imull %2, %1\n\t"
        "movl %1, %0"
        : "=r"(result)        // Output: any register
        : "r"(a), "r"(b)      // Inputs: any registers
        : "cc"                // Clobbers: condition codes
    );
    return result;
}
```

### Constraint Types

| Constraint | Meaning |
|------------|---------|
| `"r"` | Any general register |
| `"a"` | %eax/%rax register |
| `"b"` | %ebx/%rbx register |
| `"c"` | %ecx/%rcx register |
| `"d"` | %edx/%rdx register |
| `"m"` | Memory operand |
| `"i"` | Immediate integer |
| `"g"` | Any register, memory, or immediate |
| `"=r"` | Output in any register |
| `"+r"` | Input/output in same register |

### Named Operands

For readability, use named operands:

```c
__asm__ volatile(
    "imul %[multiplier], %[value]\n"
    : [value] "+r"(x)
    : [multiplier] "r"(y)
);
```

## Register Control

### Register Variables

Force variables into specific registers:

```c
void compute(int x, int y) {
    register int result asm("eax") = x;
    register int multiplier asm("ecx") = y;
    
    __asm__ volatile(
        "imull %%ecx, %%eax"
        : "+r"(result)
        : "r"(multiplier)
    );
}
```

### Global Register Variables

Reserve a register globally (use with caution):

```c
register int *stack_ptr asm("r15");  // Reserve r15 globally
```

### Local Register Variables

```c
void fast_multiply(int x) {
    // Force x into specific register for entire function scope
    register int val asm("ebx") = x;
    
    // Now ebx contains x for duration of function
    __asm__ volatile("imull $2, %%ebx" : "+r"(val));
}
```

## Function Attributes

### Optimization Control

```c
// Never inline this function
__attribute__((noinline))
void critical_function() { }

// Always inline this function
__attribute__((always_inline))
inline void hot_function() { }

// Optimize for size
__attribute__((optimize("Os")))
void small_function() { }

// Optimize for speed with O3
__attribute__((optimize("O3")))
void fast_function() { }

// Disable optimization entirely
__attribute__((optimize("O0")))
void debug_function() { }
```

### Calling Convention Control

```c
// Use Microsoft x64 calling convention
__attribute__((ms_abi))
void windows_function(int a, int b);

// Use System V AMD64 ABI
__attribute__((sysv_abi))
void linux_function(int a, int b);

// Force register parameters
__attribute__((regparm(3)))  // First 3 params in registers
void register_params(int a, int b, int c);
```

### Hot/Cold Path Hints

```c
// Mark function as frequently called
__attribute__((hot))
void frequently_called() { }

// Mark function as rarely called
__attribute__((cold))
void error_handler() { }

// Mark specific code paths
if (__builtin_expect(error_condition, 0)) {  // 0 = unlikely
    handle_error();
}
```

## Data Type Control

### Alignment Control

```c
// Align struct to cache line
struct __attribute__((aligned(64))) cache_aligned {
    int data;
};

// Pack struct tightly
struct __attribute__((packed)) tight_struct {
    char a;
    int b;
    char c;
};
```

### Vector Types

```c
// Define SIMD vector types
typedef int v4si __attribute__ ((vector_size (16)));  // 4 x 32-bit ints

v4si add_vectors(v4si a, v4si b) {
    return a + b;  // Generates SIMD add instruction
}
```

## Compiler Flags

### Optimization Flags

```bash
# Basic optimization levels
-O0  # No optimization (easiest to predict)
-O1  # Basic optimization
-O2  # Standard optimization (default for release)
-O3  # Aggressive optimization
-Os  # Optimize for size
-Ofast  # O3 + fast math

# Specific optimizations
-funroll-loops           # Unroll loops
-fno-unroll-loops       # Don't unroll loops
-finline-functions      # Inline functions
-fno-inline             # Disable inlining
```

### Architecture-Specific Flags

```bash
# Target specific CPU
-march=native           # Optimize for current CPU
-march=skylake         # Target Skylake architecture
-mtune=znver2          # Tune for Zen 2

# Enable specific instruction sets
-mavx2                 # Enable AVX2
-msse4.2              # Enable SSE4.2
-mfma                 # Enable FMA instructions
```

### Code Generation Flags

```bash
# Control code generation
-fomit-frame-pointer   # Don't use frame pointer
-fno-stack-protector   # Disable stack protection
-ffast-math           # Enable fast math optimizations
-fno-exceptions       # Disable C++ exceptions
-fno-rtti            # Disable RTTI
```

## Memory Barriers and Ordering

### Compiler Barriers

```c
// Prevent compiler reordering
__asm__ volatile("" ::: "memory");

// Or use built-in
__sync_synchronize();
```

### Memory Fences

```c
// Full memory barrier
__asm__ volatile("mfence" ::: "memory");

// Store fence
__asm__ volatile("sfence" ::: "memory");

// Load fence
__asm__ volatile("lfence" ::: "memory");
```

## Practical Examples

### Example 1: Dependency Chain

```c
// Force dependency chain without memory access
#define DEPENDENCY_CHAIN(x, y) \
    __asm__ volatile( \
        "imul %[src], %[dest]\n" \
        "imul %[src], %[dest]\n" \
        "imul %[src], %[dest]\n" \
        "imul %[src], %[dest]\n" \
        "imul %[src], %[dest]\n" \
        : [dest] "+r"(x) \
        : [src] "r"(y) \
    )
```

### Example 2: Cycle Counter

```c
static inline uint64_t rdtsc(void) {
    uint32_t lo, hi;
    __asm__ volatile(
        "rdtsc"
        : "=a"(lo), "=d"(hi)
    );
    return ((uint64_t)hi << 32) | lo;
}

// Serializing version
static inline uint64_t rdtscp(void) {
    uint32_t lo, hi, aux;
    __asm__ volatile(
        "rdtscp"
        : "=a"(lo), "=d"(hi), "=c"(aux)
    );
    return ((uint64_t)hi << 32) | lo;
}
```

### Example 3: Atomic Operations

```c
int atomic_add(int *ptr, int value) {
    int result;
    __asm__ volatile(
        "lock xaddl %0, %1"
        : "=r"(result), "+m"(*ptr)
        : "0"(value)
        : "memory"
    );
    return result;
}
```

### Example 4: CPUID

```c
void cpuid(int leaf, int subleaf, 
           int *eax, int *ebx, int *ecx, int *edx) {
    __asm__ volatile(
        "cpuid"
        : "=a"(*eax), "=b"(*ebx), "=c"(*ecx), "=d"(*edx)
        : "a"(leaf), "c"(subleaf)
    );
}
```

## Debugging and Verification

### Viewing Assembly Output

```bash
# Generate assembly
clang -S -O2 code.c -o code.s

# Generate with source intermixed
clang -S -O2 -fverbose-asm code.c

# Intel syntax instead of AT&T
clang -S -O2 -masm=intel code.c

# With debugging info
clang -S -O2 -g code.c
```

### Objdump Analysis

```bash
# Disassemble object file
objdump -d program.o

# With source intermixed
objdump -S program.o

# Intel syntax
objdump -M intel -d program.o
```

### Compiler Explorer

Use https://godbolt.org to experiment with different compilers and flags.

## Advanced Techniques

### Template Assembly (GCC/Clang)

```c
// Multiple instruction template
__asm__ volatile(
    ".rept 1000\n\t"
    "nop\n\t"
    ".endr"
);
```

### Conditional Assembly

```c
#ifdef __x86_64__
    __asm__ volatile("movq %0, %%rax" : : "r"(value));
#else
    __asm__ volatile("movl %0, %%eax" : : "r"(value));
#endif
```

### Label Usage

```c
__asm__ volatile(
    "1:\n\t"
    "decl %0\n\t"
    "jnz 1b"  // Jump back to label 1
    : "+r"(counter)
);
```

## Best Practices

1. **Start Simple**: Begin with basic inline assembly and gradually add complexity
2. **Check Output**: Always verify generated assembly matches expectations
3. **Measure Performance**: Use profiling tools to confirm improvements
4. **Document Intent**: Comment why specific assembly is needed
5. **Consider Portability**: Use `#ifdef` for architecture-specific code
6. **Minimize Inline Assembly**: Let the compiler optimize when possible
7. **Use Intrinsics**: Consider compiler intrinsics before raw assembly

## Common Pitfalls

1. **Missing Clobbers**: Always specify modified registers/memory
2. **Wrong Constraints**: Ensure constraints match instruction requirements
3. **Optimizer Conflicts**: `volatile` may be needed to prevent removal
4. **ABI Violations**: Respect calling conventions and preserved registers
5. **Undefined Behavior**: Some optimizations may break with inline assembly

## Further Reading

- [GCC Inline Assembly HOWTO](https://www.ibiblio.org/gferg/ldp/GCC-Inline-Assembly-HOWTO.html)
- [Clang Language Extensions](https://clang.llvm.org/docs/LanguageExtensions.html)
- [Intel Intrinsics Guide](https://software.intel.com/sites/landingpage/IntrinsicsGuide/)
- [Agner Fog's Optimization Manuals](https://www.agner.org/optimize/)